Overview
The goal of this competition is to create open-source algorithms and models that can solve olympiad-level math problems written in LaTeX format. Your participation will help to advance AI models’ mathematical reasoning skills and drive frontier knowledge.

Start

18 days ago
Close
4 months to go
Merger & Entry
Description
Note: This is the third AIMO Progress Prize competition. It builds upon the first AIMO Progress Prize competition, which was won in July 2024 by Project Numina and the second AIMO Progress Prize competition, which was won in April 2025 by Nvidia’s NemoSkills team. This third competition features a significantly increased problem difficulty, new submission format, expanded prize pool, industry-leading compute resources for participants, new auxiliary prizes to reward community contributions, and updated rules for using open-source LLMs.

The ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area.

The AI Mathematical Olympiad (AIMO) Prize is a $10mn fund to spur the open development of AI models capable of performing as well as top human participants in the International Mathematical Olympiad (IMO).

Recent breakthroughs demonstrate AI achieving human-level results on IMO problems. Closed-source models achieved gold medal performance at the 2025 IMO, demonstrating that frontier AI systems are now capable of solving the world's most challenging high school mathematics problems. Our "OpenAI x AIMO eval" in March 2025 revealed that commercial models could solve 50/50 AIMO2 public leaderboard problems with sufficient compute—despite the highest Kaggle score being just 34/50. This demonstrates a significant gap between closed-source and open-source models in mathematical reasoning capabilities.

AIMO3 is designed to accelerate open-source progress and close this gap. The competition features 110 problems spanning algebra, combinatorics, geometry, and number theory, ranging from National Olympiad level all the way up to IMO standard—the pinnacle of high school mathematical achievement. All problems are entirely original, created by an international team of problem solvers to ensure zero risk of train-test contamination. They have been carefully designed to be 'AI hard', tested against current open LLMs' capabilities. They require genuine mathematical reasoning to solve, making answer-only testing as robust as possible (see Problem 10 in the Reference Problems PDF for a detailed discussion of this design philosophy).

Every problem is entirely original, ensuring zero risk of data contamination. Using this transparent and fair evaluation framework, the competition will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills. This latest AIMO Progress Prize competition offers an exciting opportunity to drive innovation in the field of AI for Math, while also fostering healthy competition and supporting open science.

Join us as we work towards a future where AI models’ mathematical reasoning skills are accurately and reliably assessed, driving progress and innovation.

Evaluation
During the submission period, submission notebooks are run only over the public test set and are evaluated by the unnormalized accuracy between their predicted labels and the ground-truth labels (i.e. number of correct answers).

After the submission deadline, submission notebooks will be run twice over the private test set and their predictions concatenated into a single submission file. We then evaluate submissions by a penalized accuracy score, as follows:

If both predicted answers for a problem are correct, the score is 1 for that problem.
If one predicted answer is correct and the other is incorrect, the score is 0.5 for that problem.
If neither predicted answer is correct, the score is 0 for that problem.
A submission's overall score is the sum of its scores for each problem.

In this competition, every ground-truth label is an integer between 0 and 99999, inclusive. Any modulo calculation required is explicitly stated in the problem statement (e.g., "What is the remainder when 
 is divided by 
?"), meaning all problems have answers in this range without any further adjustments.

Note: This is a change from the first and second Progress Prizes, which used 3-digit answers with an implicit requirement to take your answer modulo 1000.

Answers may require basic computations, including modular arithmetic. For example, 
 and 
. Additional examples of computations that may be required are provided in the Reference Problems PDF found on the Data page.

Submitting
You must submit to this competition using the provided Python evaluation API, which serves test set instances one-by-one in random order for the public leaderboard, and uses fixed, random order for the private leaderboard. To use the API, follow the template in this notebook.

Prizes
TOTAL FUND FOR PROGRESS PRIZE 3: $2,207,152
Prizes for Top-Ranking Teams in this Competition:
1st Place: $262,144
2nd Place: $131,072
3rd Place: $65,536
4th Place: $32,768
5th Place: $16,384
Overall Progress Prize Winner: The Overall Progress Prize Winner shall be the highest ranking team that achieves a score of at least 47/50 on both public and private test sets. After any prizes for the five top-ranking teams have been awarded, the remainder of the total fund shall be awarded to the Overall Progress Prize Winner.

If a team is named the Overall Progress Prize Winner in this competition, the prize will be at least $1,589,248. If no team is named the Overall Progress Prize Winner in this competition, the remainder of the total fund shall roll over to the next competition, where the same prize allocation will apply.

Additional Prizes: $110,000
Longest Leader Prize: $20,000
Hard Problem Prize: $30,000
Math Corpus Prize: $30,000
Writeup Prizes: 2x $15,000
Longest Leader Prize: $20,000
Awarded to the team whose notebook(s) generates the best scoring submission on the leaderboard for the longest period of time between November 20, 2025 and February 2, 2026 11:59 PM UTC. The notebooks need to adhere to the same requirements and restrictions regarding licensing, reproducibility, and documentation to which the winning Submission is subject (see Competition Rules). The notebook(s) and any datasets used must be made publicly available at latest at February 9, 2026 11:59 PM UTC and kept public until the final Progress Prizes are awarded to the winning Teams at the end of the competition. In the event that a team has multiple public notebooks, each achieving the top leaderboard position at different times, those durations will be combined.

Hard Problem Prize: $30,000
Awarded to the highest ranked team(s) on the private leaderboard who solved the most difficult problem(s) in the private test set. Problem difficulty is measured by the average accuracy score across all selected submissions at the end of the competition, where average accuracy is calculated as described in the Evaluation section (accounting for both runs of each submission). For example, if the most difficult problem has an average accuracy of 1.7%, then the highest ranked team on the private leaderboard who achieved a full score on that problem will receive the prize. If multiple problems tie for "most difficult": We will identify the highest ranked team for each tied problem separately, and the $30,000 prize will be split equally among the winning teams. For example, if two problems tie for most difficult, each with its own highest-scoring solver, the prize will be divided into two $15,000 awards. In the event of a tie among notebooks, the tiebreaker will be based on submission time (first to submit).

Math Corpus Prize: $30,000
Awarded to the team who provides the most valuable dataset to the community for improving AIMO model performance.

Math Corpus Prize Requirements

The dataset must be publicly released either on Kaggle or on HuggingFace prior to February 9, 2026 11:59 PM UTC. Teams must create a Kaggle Discussion post until this date that explicitly links to their dataset and tags it as an entry for the Math Corpus Prize. All eligible datasets will be judged according to the criteria below, and the highest scoring dataset will be awarded the prize.

Your Kaggle Discussion post will be used to judge your dataset submission based on the criteria below, and your claims will be independently verified by the Host Team.

The dataset must be in English.

The earliest timestamped version of the dataset will be used for evaluation (later modifications will not be considered)

The dataset is not allowed to exceed 5M datapoints, which is an upper bound on common datasets used by prior AIMO winners to fine-tune their models. Each datapoint is not allowed to exceed more than 50k characters.

Each datapoint from the submitted dataset must come with an open source license that allows free dissemination of data.

Math Corpus Prize Evaluation Criteria:

Data Novelty (25 points)
Is the dataset distinctly unique from other datasets found on the internet? Minor modification are not considered novel. In particular, translations into English from other languages are not considered significant modifications.
Describe what methods you used to ensure your dataset is novel, and note how your dataset does not overlap with prior submitted datasets.
Format (25 points)
Does the dataset comes in a format that makes it easy to handle and train a model?
Does each datapoint contain rich metadata?
Performance (50 points)
Does the dataset improve mathematical reasoning and aid in improving model performance during the AIMO3. How so and how much?
Writeup Prize: $30,000 (2 awards x $15,000)
Awarded to the two best Solution Writeups from non-winning teams based on the below criteria.

Writeup Prize Requirements

The Writeup should be an official Kaggle Writeup, attached to your submission within one week after the competition ended.
It should mirror an academic publication about your submission, and include graphs that outline how your model was trained, the impact of different techniques and methods on performance, and how it could be further improved. (The arXiv post by the AIMO2 winners is a good reference paper in terms of academic standard: https://arxiv.org/abs/2504.16891)
It should be limited to 30k words (which approximately corresponds to a 50-page paper), and split into a main section of not more than 5k words (approximately 10 pages), which outlines your methodology, and the remaining sections, which include the bibliography, examples, and any other materials that support the main section.
Writeup Prize Evaluation Criteria

Clarity (10 points)
Does the Writeup describe in detail the entire model creation lifecycle (including data collection, training, and benchmarking), as well as any model variants you explored?
Ablation Studies and Variants (10 points)
Does the Writeup contain information that pinpoints where the performance gains of your model came from?
Comparison with the State of the Art (10 points)
Does the Writeup evaluate how your model (potentially split across types of reasoning modes or mathematical problems) compares to the state of the art for both open-weight and commercial models?
Graphs and Charts (10 points)
Does the Writeup contain well-designed visuals that clearly "tell the story" by illustrating your model's performance, training process, and key experimental results?
Reproducibility (60 points)
Is your write-up sufficiently detailed to easily allow the reader to reproduce the training of your model starting from the same artifacts (base open-weight models and data) that you started from?
Timeline
November 20, 2025 - Start Date.
April 8, 2026 - Entry Deadline. You must accept the competition rules before this date in order to compete.
April 8, 2026 - Team Merger Deadline. This is the last day participants may join or merge teams.
April 15, 2026 - Final Submission Deadline.
As noted in the Evaluation section, selected submissions will be run against the private test set after the Submission deadline. Final results will be available shortly thereafter.

All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.

Code Requirements


Submissions to this competition must be made through Notebooks. In order for the "Submit" button to be active after a commit, the following conditions must be met:

CPU Notebook <= 9 hours run-time
GPU Notebook <= 5 hours run-time
Internet access disabled
Freely & publicly available external data is allowed, including pre-trained models
Submission file must be generated by the API.
Submission runtimes have been obfuscated. If you repeat the exact same submission you will see up to 30 minutes of variance in the time before you receive your score.

Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors.

Upgraded Kaggle Hardware
We're excited to announce that we've added powerful new H100 machines to the AIMO hardware pool! See this page for more on these machines.

What you need to know:

AIMO 3 Only - H100 are only available for notebooks attached to this competition, use of H100s for any other activity could result in moderation action (e.g. site suspension or account ban).
No Internet - To help ensure these machines are reserved for competition use, all H100 sessions must have internet disabled.
Note on Language and Notation
All problems are text-only with mathematical notation in LaTeX. The following conventions are observed:

, 
 and 
 packages are used. No other package is assumed.
Variable names are given in math mode e.g. 
, 
, 
. 
 letters may be used, e.g. 
, 
.
Integers such as 1, 2 can be outside of math mode or inside. Large integers such as 
 are in math mode.
 and 
 environments may be used (with the latter using the conventions from the 
 package).
, 
, 
, 
, and 
 may all be used for equations.
No environments other than the ones mentioned above are used.
, 
, 
 can be used to continue a finite sequence or infinite.
Multiplication can be represented by simple adjacency 
 or using LaTeX 
 command for 
 or LaTeX 
 command for 
.
 for an integer 
 represents the factorial function 
 (with 
).
, 
, 
 are generally in math mode. Unless a base is explictly stated (e.g., $\log_{2}$), $\log$ refers to the natural logarithm.
Fractions may be represented using 
 or inline using 
.
 represents the absolute value of 
, e.g., 
.
, where 
 is a real number, represents the value 
Standard geometric notation is used such as the triangle 
 with sides 
, 
, and 
. Standard notation is also used for angles, e.g., 
. Angles may be specified in degrees 
. Measures with no units should be assumed to be in radians.
Standard trigonometric operators including 
, 
, 
 may be used.
Sets are denoted using standard curly bracket notation, e.g., 
.
 represents the integer part of the real number 
 ie the greatest integer less than or equal to 
. 
 represents the smallest integer greater than or equal to 
. 
 for a single value 
 refers to the fractional part of the real number 
 ie 
.
 and 
 may be used in conjunction with other LaTeX commands, e.g. 
 producing 
.
Overline notation 
 represents the integer formed by the digits 
,
,
,
 with 
, e.g. with 
, 
: 
 and 
.
 is interpreted as the binomial coefficient, representing the number of ways of choosing 
 elements from a set of size 
. We use the convention that 
 and 
 if 
.
, and 
 are used to denote the set of positive integers, integers, rational numbers, and real numbers, respectively (where positive means 
). For an integer 
, the notation 
 may be used to denote the set of integers 
.
Standard notation for functions will be used such as 
 where 
 and 
 are sets.
We allow informal probability language such as: a point is chosen uniformly at random in the interval 
.
The sum over the empty set is 
, and the product over the empty set is 
.
For integers 
, 
, and 
, 
 denotes 
.
British or American versions of English can be used. Thus ''highest common factor'' means the same as ''greatest common divisor''.
A prefix or subscript may be used to indicate features of a triangle associated with vertices. Thus triangle 
 has three altitudes, and the one dropped from 
 could be denoted the altitude through 
, the 
-altitude or the altitude 
. Similarly, for median lines.
 means 'is defined to be equal to'.
Taxonomy is Bourbakist: equilateral triangles are isosceles triangles, squares are rectangles and rectangles are parallelograms.
The word trapezium can mean different things in different countries. For the purposes of this competition, a trapezium is a quadrilateral with at least one pair of parallel opposite sides.
Please read this section carefully as there have been changes from the answer format used for AIMO2. We recommend looking through the reference problems to see examples.

All answers must be reported as integers in the range 
 to 
 inclusive.
All questions are worded so that the final answer will lie in this range (meaning any modulus required is explicitly given in the problem statement).
For questions of the form: `What is the remainder when 
 is divided by 
?' where 
 is a positive integer and 
 is an integer, you should return the unique integer 
 such that 
 and 
.
Answers may require basic computations, e.g. 
. This includes modular arithmetic, e.g. 
.
Partnerships and Additional Resources
The Hosts' aim is not just to run a competition, but to provide an ecosystem for it.

We thus have partnered with the Fields Model Initiative (supported by two organizations: the LLMC from the National Institute of Informatics in Tokyo and Benchmarks+Baselines from Vienna) and with Thinking Machines, which makes the Tinker service available.

These partnerships provide compute and API credits, respectively, to help contestants train/fine-tune their models. AIMO is the first large-scale Kaggle reasoning competition to provide significant support in this direction, enabling competitors to have a streamlined experience for training and competing with their models.

This addresses a significant pain point from previous AIMO competitions and a persistent issue of any large-scale competitions that focus on computationally expensive AI models, such as LLMs, as previous competitions were 1) skewed towards large labs with significant resources having an edge, and 2) skewed towards teams with strong engineering know-how, putting mathematicians keen on competing at a disadvantage.

Our partnerships solve both problems. AIMO3 levels the playing field by providing access to 128 H100 GPUs via the Fields Model Initiative (applications for more will be considered on a case-by-case basis), totaling hundreds of thousands of hours of compute we can offer to teams, and up to $400 in API credit for Tinker. Tinker abstracts away much of the engineering knowledge, making it particularly beneficial for mathematicians.

Because demand may exceed supply, an application process will be in place for both of these services, and applications will open later in December, with precise details outlined in the Kaggle Discussion forum. Because it is our goal to make these resources available to teams with the greatest promise to devise a strong reasoning model, factors beyond application time may be considered.

About the Hosts
XTX Markets is a leading algorithmic trading company and has over 200 employees based in London, Paris, New York, Mumbai, Yerevan and Singapore. XTX provides liquidity in the Equity, FX, Fixed Income and Commodity markets and trades over $250bn a day across markets.

XTX Markets' expansive research cluster contains 100,000 cores and 20,000 A/V100 GPUs and is growing. It also has 390 petabytes of usable storage and 7.5 petabytes of RAM. Alongside rich datasets and advanced technological infrastructure we are at the forefront of the crossover of finance and technology.

XTX Markets’ philanthropy focuses on maths and science education and research, alongside other areas such as academic sanctuaries, carbon removal and an employee matching programme. Since 2017, XTX Markets has donated over £100mn to charities and good causes, establishing it as a major donor in the UK and globally.

Citation
Simon Frieder, Sam Bealing, Philip Vonderlind, Sida Li, Arsenii Nikolaiev, Geoff C. Smith, Kevin Buzzard, Timothy Gowers, Peter J. Liu, Po-Shen Loh, Lester Mackey, Leonardo de Moura, Dan Roberts, D. Sculley, Terence Tao, David Balduzzi, Simon Coyle, Alex Gerko, Ryan Holbrook, Addison Howard, and XTX Markets. AI Mathematical Olympiad - Progress Prize 3. https://kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-3, 2025. Kaggle.


Dataset Description
The competition data comprises 110 mathematics problems similar in style to those of the AIME.

The answer to each problem is a non-negative integer between 0 and 99999, inclusive.

You should expect the problems to be roughly ranging in difficulty from a national Olympiad to the International Mathematical Olympiad (IMO), with some problems being slightly easier and some being slightly harder.

All problems are text-only with mathematical notation in LaTeX. Please see the Overview, Section Note on Language and Notation for details on the notational conventions used.

Although some problems may involve geometry, diagrams are not used in any problem.

The public test set comprises exactly 50 problems, and the private test set comprises a further distinct set of 50 problems. We also provide a selection of 10 problems for use as reference, called "reference data". A pdf with full solutions to the problems from the reference data can be found below. The problems in the private and public sets have been selected to balance both difficulty and subject area. Their difficulty and subject distribution is similar.

Competition Data
Please note that this is a Code Competition. You must submit to this competition using the provided Python evaluation API, which serves the test set question by question in random order. To use the API, follow the example in this notebook: AIMO 3 Submission Demo.

We give a few placeholder problems in the publicly visible test.csv file to help you author your submissions. These problems are not representative of the problems in the actual test set.

When your submission is scored, this placeholder test data will be replaced with the actual test data.

Private Set Rerun
Because of the limited number of problems available, we are taking special precautions to secure the test set against probing. Among other things, during the submission period, the test set will comprise only the 50 public set problems. Once the competition ends, we will rerun all model submissions twice on the 50 private set problems. The overall score is an average accuracy score as described on the Evaluation page. You should attempt to make sure your submission will complete successfully on the 50 new private set problems. This may mean ensuring your submission is robust to unexpected inputs, or managing runtime and memory usage. A submission must complete successfully on both private set reruns or it will not be scored.

Files
reference.csv - Contains 10 problems for use as reference data.
test.csv - Contains 50 problems. Please note that the problems visible here are only placeholders. Your submission will have access to the full set of problems during scoring.
sample_submission.csv - A sample submission file in the correct format. See the Evaluation page for more information on the submission format.
Fields
id - A unique identifier for each problem.
problem - A LaTeX statement of the problem to be solved.
answer - An integer from 0 to 99999, inclusive.
Files
15 files

Size
741.26 kB

Type
py, csv, pdf + 1 other

License
CC BY-SA 4.0

AIMO3_Reference_Problems.pdf(676.7 kB)

Data Explorer
741.26 kB

kaggle_evaluation

AIMO3_Reference_Problems.pdf

reference.csv

sample_submission.csv

test.csv

Summary
15 files

7 columns


Download All
kaggle competitions download -c ai-mathematical-olympiad-progress-prize-3
Download data

Metadata


AIMO 3 Submission Demo



import os

import kaggle_evaluation.aimo_3_inference_server
import pandas as pd
import polars as pl


class Model:
    """A dummy model."""

    def __init__(self):
        self._model = None

    def load(self):
        """Simulate model loading."""
        print("Loading model...")
        # Just return a "model" that always answers with 0
        return lambda problem: 0

    def predict(self, problem: str):
        # Employ lazy loading: load model on the first model.predict call
        if self._model is None:
            self._model = self.load()
        return self._model(problem)


model = Model()


# Replace this function with your inference code.
# The function should return a single integer between 0 and 99999, inclusive.
def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:
    """Make a prediction."""
    # Unpack values
    id_ = id_.item(0)
    problem_text: str = problem.item(0)
    # Make a prediction
    # The model is loaded on the first call
    prediction = model.predict(problem_text)
    return pl.DataFrame({'id': id_, 'answer': prediction})


inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(
    predict
)

if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):
    # You MUST call this within 15 minutes of the script starting. This is to
    # ensure a "fast fail" in case a bug prevents the inference server from starting.
    # Do anything that might take a long time (like model loading) in the predict
    # function, which has no time limit.
    inference_server.serve()
else:
    inference_server.run_local_gateway(
        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)
    )