# MathLLM ë‚¨ì€ ì‘ì—… ê³„íš

## í˜„ì¬ ìƒíƒœ ìš”ì•½

| í•­ëª© | ìƒíƒœ | ê²°ê³¼ |
|------|------|------|
| Qwen Zero-shot baseline | âœ… ì™„ë£Œ | **93.71%** (GSM8K) |
| TRM Identity Test | âœ… ì™„ë£Œ | Zero Init ê²€ì¦ í†µê³¼ |
| TRM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ | âœ… ì™„ë£Œ | `train_trm.py` |
| Finetune í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ | âœ… ì™„ë£Œ | `train_finetune.py` |
| LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ | âœ… ì™„ë£Œ | `train_lora.py` |
| TRM í•™ìŠµ | ğŸ”„ ì§„í–‰ì¤‘ | Loss 7.08 â†’ 2.2 (Step 100) |

---

## ë‚¨ì€ ì‘ì—…

### Phase 2: Core Experiment (í˜„ì¬ ì§„í–‰ì¤‘)

#### 2.1 TRM í•™ìŠµ ì™„ë£Œ ë° í‰ê°€
- [ ] TRM í•™ìŠµ ì™„ë£Œ ëŒ€ê¸° (ì˜ˆìƒ ~18ì‹œê°„)
- [ ] í•™ìŠµëœ TRM ëª¨ë¸ë¡œ GSM8K í‰ê°€
- [ ] ê²°ê³¼ ê¸°ë¡

#### 2.2 Finetune Baseline í•™ìŠµ
```bash
uv run python train_finetune.py \
  --n_layers 1 \
  --dataset gsm8k \
  --epochs 3 \
  --batch_size 4 \
  --output_dir ./checkpoints/finetune_1layer
```
- [ ] Last 1 layer finetune í•™ìŠµ (~233M params)
- [ ] GSM8K í‰ê°€
- [ ] TRMê³¼ ë¹„êµ

#### 2.3 (Optional) LoRA Baseline
```bash
uv run python train_lora.py \
  --rank 64 \
  --dataset gsm8k \
  --epochs 3 \
  --output_dir ./checkpoints/lora
```
- [ ] LoRA í•™ìŠµ (~160M params)
- [ ] GSM8K í‰ê°€

---

### Phase 3: Extreme Test (Dynamic T)

#### 3.1 Inference-time Depth Scaling
- [ ] `src/model.py`ì˜ `generate()` ë©”ì„œë“œì— dynamic T ì§€ì› ì¶”ê°€
- [ ] T=3 â†’ T=5 â†’ T=10ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ë©° í…ŒìŠ¤íŠ¸
- [ ] ì–´ë ¤ìš´ ë¬¸ì œ(GSM8K hard subset)ì—ì„œ ì •ë‹µë¥  ë³€í™” ì¸¡ì •

#### 3.2 MATH Dataset í‰ê°€
- [ ] `eval/math_eval.py` êµ¬í˜„
- [ ] MATH Level 5 ë¬¸ì œì—ì„œ TRM vs Finetune ë¹„êµ
- [ ] Test-time compute íš¨ê³¼ ê²€ì¦

---

### Phase 4: Kaggle Submission ì¤€ë¹„

#### 4.1 Inference ìµœì í™”
- [ ] 5ì‹œê°„ GPU ì œí•œ ë‚´ 50ë¬¸ì œ í•´ê²° ê°€ëŠ¥í•œì§€ í™•ì¸
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- [ ] vLLM ë˜ëŠ” TensorRT ì ìš© ê²€í† 

#### 4.2 Submission Format
- [ ] Kaggle notebook í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- [ ] ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë¡œë”© (ì¸í„°ë„· ë¹„í™œì„±í™” í™˜ê²½)
- [ ] ë‹µë³€ í˜•ì‹ ê²€ì¦ (0-99999 ì •ìˆ˜)

#### 4.3 ìµœì¢… í…ŒìŠ¤íŠ¸
- [ ] AIMO Public ë¬¸ì œë¡œ í…ŒìŠ¤íŠ¸
- [ ] 2íšŒ ì‹¤í–‰ ì¼ê´€ì„± í™•ì¸

---

## ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ëŒ€ì•ˆ

í˜„ì¬ TRM í•™ìŠµì´ ~18ì‹œê°„ ì†Œìš” ì˜ˆìƒ. ë¹ ë¥¸ ê²€ì¦ì„ ì›í•˜ë©´:

### Option A: ìƒ˜í”Œ ìˆ˜ ì œí•œ
```bash
uv run python train_trm.py \
  --num_samples 1000 \
  --epochs 1 \
  --N_supervision 16 \
  --output_dir ./checkpoints/trm_quick
```
ì˜ˆìƒ ì‹œê°„: ~1ì‹œê°„

### Option B: N_supervision ê°ì†Œ
```bash
uv run python train_trm.py \
  --N_supervision 4 \
  --epochs 1 \
  --output_dir ./checkpoints/trm_n4
```
ì˜ˆìƒ ì‹œê°„: ~4-5ì‹œê°„

### Option C: T_recursion ê°ì†Œ
```bash
uv run python train_trm.py \
  --T_recursion 1 \
  --epochs 1 \
  --output_dir ./checkpoints/trm_t1
```
ì˜ˆìƒ ì‹œê°„: ~6ì‹œê°„

---

## ê²°ê³¼ ê¸°ë¡ í…œí”Œë¦¿

| ëª¨ë¸ | í•™ìŠµ ë°ì´í„° | Params | GSM8K | MATH Lvl5 | ë¹„ê³  |
|------|------------|--------|-------|-----------|------|
| Qwen-7B (Zero-shot) | - | 0 | 93.71% | - | ê¸°ì¤€ì  |
| Qwen + TRM | GSM8K | 176M | % | % | ì‹¤í—˜êµ° |
| Qwen + Finetune (1 layer) | GSM8K | ~233M | % | % | ëŒ€ì¡°êµ° |
| Qwen + LoRA (r=64) | GSM8K | ~160M | % | % | ëŒ€ì¡°êµ° |

---

## í•µì‹¬ ì§ˆë¬¸

> **"TRMì´ ë‹¨ìˆœí•œ íŒŒë¼ë¯¸í„° ì¦ëŸ‰ì´ ì•„ë‹ˆë¼, ì¬ê·€ì  êµ¬ì¡° ë•ë¶„ì— ì„±ëŠ¥ì´ ì˜¤ë¥´ëŠ”ê°€?"**

ì„±ê³µ ì¡°ê±´:
- `Score(TRM) > Score(Finetune)` with ë¹„ìŠ·í•œ param ìˆ˜
- T ì¦ê°€ ì‹œ ì–´ë ¤ìš´ ë¬¸ì œ ì •ë‹µë¥  ìƒìŠ¹ (Test-time compute íš¨ê³¼)
