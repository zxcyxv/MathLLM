Gemini의 답변 : 구현 명세서: Qwen-TRM Integrated ModelPart 2. The Recursive Reasoning Engine1. Conceptual Design: The Single Network LogicTRM의 가장 큰 이론적 특징은 두 개의 네트워크($f_L, f_H$)를 사용하는 HRM과 달리, 단 하나의 **Tiny Network ($f_\theta$)**가 입력 구성에 따라 두 가지 다른 역할(Reasoning vs Prediction)을 수행한다는 점입니다.우리는 이 단일 네트워크가 Information Bottleneck 원리에 입각하여 다음과 같이 작동하도록 설계합니다.A. Dual-Mode Operation (입력 마스킹을 통한 모드 전환)네트워크 $f_\theta$는 입력 벡터의 조합 $[x, y, z]$를 받아 상태를 갱신합니다. 이때 $x$(Context)의 포함 여부가 연산의 성격을 결정합니다.Reasoning Mode (Update $z$):입력: $[x; y; z]$ (Context + Current Answer + Reasoning Path)수식: $z_{k+1} = f_\theta(x, y_k, z_k)$의미: 문제의 문맥($x$)을 계속 참조하며 추론의 궤적($z$)을 수정합니다.Prediction Mode (Update $y$):입력: $[\vec{0}; y; z]$ (Context Masked + Current Answer + Reasoning Path)수식: $y_{k+1} = f_\theta(\vec{0}, y_k, z_{new})$의미: 문맥 $x$를 직접 보지 않고, 오직 추론된 결과 $z$에 의존하여 해답 $y$를 생성합니다. 이는 $z$가 $x$의 정보를 충분히 압축하고 소화했는지를 강제하는 강력한 Bottleneck 역할을 합니다.2. Architecture Specification: The "Tiny" Block"Tiny"의 정의에 따라 오버피팅을 방지하고 일반화 성능을 높이기 위해, 네트워크는 2-Layer MLP 구조를 채택합니다.Structure: Linear $\rightarrow$ SwiGLU $\rightarrow$ Linear $\rightarrow$ RMSNormInput Dimension: $3 \times D_{lat}$ (Concatenation of $x, y, z$)Hidden Dimension: $D_{ff}$ (일반적으로 $4 \times D_{lat}$ 또는 논문의 비율에 따름)Output Dimension: $D_{lat}$3. Implementation: TinyRecursiveEngine이 클래스는 실제 재귀 루프를 관리합니다. forward 메서드는 한 번의 호출로 끝나는 것이 아니라, 내부적으로 $n$번의 루프를 돕니다.Pythonimport torch
import torch.nn as nn
import torch.nn.functional as F

class SwiGLU(nn.Module):
    """
    SwiGLU Activation as used in the paper[cite: 64].
    FFN(x) = (Swish(xW) * xV)W2
    """
    def __init__(self, in_features, hidden_features, out_features, bias=False):
        super().__init__()
        # We need two linear projections for the gate and value
        self.w1 = nn.Linear(in_features, hidden_features, bias=bias) # Gate
        self.w2 = nn.Linear(in_features, hidden_features, bias=bias) # Value
        self.w3 = nn.Linear(hidden_features, out_features, bias=bias) # Output

    def forward(self, x):
        # Swish(xW) * xV
        x1 = F.silu(self.w1(x))
        x2 = self.w2(x)
        return self.w3(x1 * x2)

class TinyRecursiveEngine(nn.Module):
    def __init__(self, d_lat=1024, d_ff=None, n_recursion=6):
        """
        Args:
            d_lat: Latent dimension size (default 1024 from Part 1)
            d_ff: Intermediate hidden size (default 4 * d_lat)
            n_recursion: Number of latent recursion steps (n in paper)
        """
        super().__init__()
        self.d_lat = d_lat
        self.d_ff = d_ff or 4 * d_lat
        self.n = n_recursion

        # The Single Tiny Network [cite: 8, 269]
        # Input: Concat(x, y, z) -> 3 * d_lat
        self.net = nn.Sequential(
            SwiGLU(3 * d_lat, self.d_ff, d_lat),
            nn.RMSNorm(d_lat) # Norm helps stability in recursive loops
        )

        # Optional: EMA Handling could be added here as per[cite: 298], 
        # but usually handled by the training loop/optimizer.

    def _single_step(self, x_in, y_in, z_in):
        """
        Performs one pass of the network f_theta.
        """
        # [Batch, Seq, 3*d_lat]
        combined = torch.cat([x_in, y_in, z_in], dim=-1)
        return self.net(combined)

    def forward_recursion_process(self, x, y, z):
        """
        Executes one full 'Recursive Process' (Algorithm 3 in paper).
        Consists of n latent updates followed by 1 solution update.
        
        Args:
            x: Context [B, S, D] (Fixed anchor)
            y: Current Solution [B, S, D]
            z: Current Reasoning [B, S, D]
            
        Returns:
            y_new, z_new: Updated states
        """
        
        # 1. Latent Recursion Loop (Reasoning Mode) [cite: 223]
        # "Recursively improving its latent z"
        # z <- f(x, y, z) repeated n times
        
        curr_z = z
        for _ in range(self.n):
            # Pass x, y, z to update z
            # Note: y is kept fixed during this inner z-refinement
            delta_z = self._single_step(x, y, curr_z)
            
            # Residual Connection is implicit in recursion if trained that way,
            # but paper implies direct replacement: z_L <- f(...)
            # However, for gradient stability in deep recursion, 
            # z_new = z_old + f(...) is often better. 
            # Let's adhere to the paper's functional form z = net(...) 
            # but check if they use residuals.
            # Paper mentions "residual connections" in HRM[cite: 48], 
            # implies TRM inherits similar stability traits.
            # We will use direct update for strict paper adherence, 
            # but add a residual option for safety.
            curr_z = delta_z + curr_z # ResNet-style update for gradient flow

        # 2. Solution Update (Prediction Mode) [cite: 224, 267]
        # y <- f(0, y, z)
        # We mask x with zeros to force dependency on z
        x_dummy = torch.zeros_like(x)
        
        # Update y using the refined z
        delta_y = self._single_step(x_dummy, y, curr_z)
        y_new = delta_y + y # Residual update for y as well
        
        return y_new, curr_z
4. Technical Analysis: Why This Maximizes Depth이 구현은 사용자님이 요구하신 **"Deep Computational Depth"**를 다음과 같은 메커니즘으로 달성합니다.Effective Depth Multiplier:이 모듈이 한 번 호출될 때(forward_recursion_process), 내부적으로 SwiGLU 연산이 $n + 1$회 수행됩니다.Deep Supervision 루프($T$)까지 고려하면, 실제 연산 깊이는 $T \times (n + 1)$ 레이어가 됩니다.Example: $T=3, n=6$ $\rightarrow$ 21 Layers of extra depth.이는 물리적인 레이어 추가 없이, 시간 축(Temporal Axis)으로 모델을 확장하는 효과를 냅니다.State-Space Trajectory:curr_z 변수는 루프를 돌 때마다 입력 $x$와 현재 해답 $y$ 사이의 관계를 재해석합니다. 초기 $z$가 얕은 수준의 연관성만 포착했다면, $n$번의 재귀를 거친 $z$는 수학적 문제 해결에 필요한 다단계 추론(Multi-step Reasoning) 정보를 인코딩하게 됩니다.Forced Reasoning via Bottleneck:Prediction Mode에서 $x$를 zeros_like로 마스킹하는 전략은 모델이 "문제를 다시 읽는 것(Reading Comprehension)"에 의존하지 않고, "이미 생각한 것(Reasoning Memory $z$)"에 의존하여 답을 내도록 강제합니다. 이는 단순 패턴 매칭을 방지하고 심층 추론을 유도하는 핵심 장치입니다.



사용자의 질문 : 2. Architecture Specification: The "Tiny" Block"Tiny"의 정의에 따라 오버피팅을 방지하고 일반화 성능을 높이기 위해, 네트워크는 2-Layer MLP 구조를 채택합니다3333.Structure: Linear $\rightarrow$ SwiGLU $\rightarrow$ Linear $\rightarrow$ RMSNormInput Dimension: $3 \times D_{lat}$ (Concatenation of $x, y, z$)Hidden Dimension: $D_{ff}$ (일반적으로 $4 \times D_{lat}$ 또는 논문의 비율에 따름)Output Dimension: $D_{lat}$



이거는 내 목표에 맞지 않는 것 같아.



일단 언어모델의 구조 특성상 Transformer를 사용하는게 훨씬 더 안전한 것 같고



어 그냥 Transformer로 바꿔


Gemini의 답변 : 사용자님의 판단이 정확합니다. 실제로 TRM 논문에서도 긴 문맥(Long Context)이나 복잡한 관계성이 필요한 태스크(ARC-AGI, Maze)에서는 MLP 대신 Self-Attention을 사용했을 때 일반화 성능이 더 좋았다고 보고하고 있습니다1111111.언어 모델(Qwen)의 특성상, 토큰 간의 상호작용(Global Dependency)을 포착하는 것이 추론에 필수적입니다. 따라서 MLP 기반의 단순 혼합(Mixing) 방식을 버리고, Transformer Layer (Attention + FFN) 구조로 변경하여 Part 2를 수정 작성하겠습니다.구현 명세서: Qwen-TRM Integrated ModelPart 2. The Recursive Reasoning Engine (Revised: Transformer Ver.)1. Revised Architecture: The "Tiny" Transformer단순 MLP를 Standard Transformer Block으로 대체합니다. 이 블록은 입력 시퀀스 내의 토큰 간 상호작용을 처리하므로, 앞뒤 문맥을 고려한 더 정교한 $z$ 갱신이 가능합니다.A. Structural Definition입력 상태 조합 $[x, y, z]$를 융합한 후, Causal Self-Attention과 SwiGLU FFN을 통과시킵니다.Input Fusion: $h_{in} = \text{Linear}_{proj}([x; y; z])$Transformer Block:$$h_{attn} = x + \text{Attention}(\text{RMSNorm}(x))$$$$h_{out} = h_{attn} + \text{FFN}(\text{RMSNorm}(h_{attn}))$$Dimensionality:Input: $3 \times D_{lat}$ (Concatenation)Model Dim: $D_{lat}$ (1024)Heads: $8 \sim 16$ (Head Dim 64 or 128)2. Implementation: TinyRecursiveTransformerPyTorch의 nn.MultiheadAttention을 사용하거나 Qwen의 Attention을 모방하여 구현할 수 있습니다. 여기서는 Qwen과의 호환성을 위해 Causal Masking이 적용된 Transformer Block을 구현합니다.Pythonimport torch
import torch.nn as nn
import torch.nn.functional as F
import math

class TRMAttention(nn.Module):
    """
    Standard Causal Self-Attention with RoPE (optional) or relative embeddings.
    For simplicity and compatibility, we use standard Scaled Dot-Product Attention.
    """
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_head = d_model // num_heads
        
        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)
        self.o_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x, attention_mask=None):
        B, S, D = x.shape
        # Q, K, V projection
        qkv = self.qkv(x).reshape(B, S, 3, self.num_heads, self.d_head).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Flash Attention (if available) or Standard implementation
        # Uses scaled_dot_product_attention which supports efficient backend
        output = F.scaled_dot_product_attention(
            q, k, v, 
            attn_mask=attention_mask, 
            dropout_p=0.0, 
            is_causal=True if attention_mask is None else False
        )
        
        # Reshape and Output projection
        output = output.transpose(1, 2).reshape(B, S, D)
        return self.o_proj(output)

class TRMTransformerBlock(nn.Module):
    """
    Single Layer Transformer Block: RMSNorm -> Attn -> Residual -> RMSNorm -> SwiGLU -> Residual
    """
    def __init__(self, d_model, num_heads, d_ff):
        super().__init__()
        self.norm1 = nn.RMSNorm(d_model)
        self.attn = TRMAttention(d_model, num_heads)
        
        self.norm2 = nn.RMSNorm(d_model)
        # SwiGLU FFN
        self.w1 = nn.Linear(d_model, d_ff, bias=False) # Gate
        self.w2 = nn.Linear(d_model, d_ff, bias=False) # Value
        self.w3 = nn.Linear(d_ff, d_model, bias=False) # Output

    def forward(self, x, attention_mask=None):
        # Attention Sub-layer
        h = x + self.attn(self.norm1(x), attention_mask)
        
        # FFN Sub-layer (SwiGLU)
        x_norm = self.norm2(h)
        gate = F.silu(self.w1(x_norm))
        value = self.w2(x_norm)
        ffn_out = self.w3(gate * value)
        
        return h + ffn_out

class TinyRecursiveTransformer(nn.Module):
    def __init__(self, d_lat=1024, num_heads=16, n_recursion=6):
        super().__init__()
        self.d_lat = d_lat
        self.n = n_recursion
        
        # 1. Input Projection (Fusion Layer)
        # Compresses [x; y; z] (3072 dim) down to d_lat (1024 dim)
        self.input_fusion = nn.Linear(3 * d_lat, d_lat, bias=False)
        
        # 2. The Core Transformer Block
        self.transformer_block = TRMTransformerBlock(
            d_model=d_lat, 
            num_heads=num_heads, 
            d_ff=4 * d_lat
        )

    def _single_step(self, x, y, z):
        """
        Fuses inputs and passes through the Transformer Block.
        """
        # [Batch, Seq, 3 * D]
        combined_input = torch.cat([x, y, z], dim=-1)
        
        # [Batch, Seq, D]
        fused = self.input_fusion(combined_input)
        
        # Apply Transformer Block (Implicitly uses Causal Mask in SDPA)
        output = self.transformer_block(fused)
        
        return output

    def forward_recursion_process(self, x, y, z):
        """
        Executes Algorithm 3: n latent steps + 1 solution step.
        """
        curr_z = z
        
        # 1. Latent Recursion Loop (Update Reasoning z)
        for _ in range(self.n):
            # z_new = Transformer(x, y, z_old)
            # Using residual update: z += f(...)
            delta_z = self._single_step(x, y, curr_z)
            curr_z = curr_z + delta_z

        # 2. Solution Update (Update Prediction y)
        # Mask x with zeros to enforce bottleneck
        x_dummy = torch.zeros_like(x)
        
        # y_new = Transformer(0, y, z_new)
        delta_y = self._single_step(x_dummy, y, curr_z)
        y_new = y + delta_y
        
        return y_new, curr_z
3. Why This Change is Safer & BetterInductive Bias of Transformers:수정된 구조는 언어 모델의 본질인 "시퀀스 처리" 능력을 계승합니다. $z$ 상태가 갱신될 때, 단순히 현재 토큰 위치의 정보($x_t$)뿐만 아니라 이전 토큰들의 추론 정보($z_{<t}$)를 참고할 수 있게 됩니다2. 이는 수학 문제 풀이와 같이 단계적 논리가 필요한 작업에서 결정적입니다.Compatibility with Qwen:Backbone인 Qwen도 Transformer 구조이므로, Feature Space의 성질이 유사하게 유지되어 학습 안정성이 높아집니다(Gradient Flow가 더 자연스러움).Enhanced Capacity:MLP에 비해 Self-Attention은 파라미터 대비 표현력이 월등히 높습니다. 비록 "Tiny"한 사이즈(1 Layer)지만, $n=6, T=3$의 재귀를 통해 약 21 Layer 분량의 Attention Map을 쌓는 효과를 냅니다.



사용자의 질문 : 음... 내용이 좀 모자란데.



아까 논의했던 KV-Cache 불일치 문제는 어떻게 해결했는지도 적어야지.


Gemini의 답변 : 구현 명세서: Qwen-TRM Integrated ModelPart 2-1. Addressing KV-Cache Mismatch (Optimization)1. The Problem: Recursive State InvalidationTRM의 입력은 $[x; y; z]$의 결합입니다.$x$ (Context): Qwen에서 넘어온 값으로, 재귀 루프($n$) 도는 동안 **불변(Static)**입니다.$y, z$ (Reasoning/Answer): 재귀 루프를 돌 때마다 **가변(Dynamic)**입니다.일반적인 Attention을 쓰면 매 루프마다 $x$에 대한 $K, V$ 프로젝션을 다시 수행해야 합니다. $x$가 시퀀스의 대부분을 차지한다면 이는 심각한 자원 낭비입니다.2. The Solution: Static-Dynamic KV Separation입력 융합(Fusion) 단계를 분리하여, $x$에 대한 KV는 캐싱(Caching)하고, $y, z$에 대한 KV만 실시간 갱신하는 하이브리드 어텐션 구조를 도입합니다.$$K_{total} = [K_{static}(x) \ ; \ K_{dynamic}(y, z)]$$$$V_{total} = [V_{static}(x) \ ; \ V_{dynamic}(y, z)]$$이렇게 하면 재귀 루프 내에서 $x$에 대한 행렬 연산을 제거하여 연산 속도를 획기적으로 높일 수 있습니다.3. Revised Implementation기존 TinyRecursiveTransformer에 Partial Caching 기능을 추가한 코드입니다.Pythonimport torch
import torch.nn as nn
import torch.nn.functional as F

class DecoupledTRMAttention(nn.Module):
    """
    Attention module that handles Static KV (from x) and Dynamic KV (from y, z) separately.
    """
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        # Projections strictly separated
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        
        # KV for Static Context (x)
        self.k_static = nn.Linear(d_model, d_model, bias=False)
        self.v_static = nn.Linear(d_model, d_model, bias=False)
        
        # KV for Dynamic State (y, z)
        self.k_dynamic = nn.Linear(d_model, d_model, bias=False)
        self.v_dynamic = nn.Linear(d_model, d_model, bias=False)
        
        self.o_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, hidden_states, static_k=None, static_v=None, attention_mask=None):
        """
        hidden_states: Current dynamic state inputs (derived from y, z) [B, S, D]
        static_k, static_v: Pre-computed KV from x [B, S, D]
        """
        B, S, D = hidden_states.shape
        
        # 1. Compute Query from current dynamic state
        q = self.q_proj(hidden_states)
        q = q.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 2. Compute Dynamic KV
        k_dyn = self.k_dynamic(hidden_states).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        v_dyn = self.v_dynamic(hidden_states).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 3. Concatenate with Static KV (The "Fix")
        # Ensure static_k/v matches the batch/head dimensions
        # K_total = [K_static, K_dynamic] along sequence dimension is NOT correct here 
        # because x, y, z are aligned element-wise (same sequence position).
        # We need to attend to ALL of them.
        
        # Correction Strategy: 
        # TRM fuses [x, y, z] -> projected to D_lat.
        # But to save compute, we want to avoid re-projecting the 'x' component of the fusion.
        # Since standard Fusion is Linear([x;y;z]), we can mathematically split it:
        # Fusion([x;y;z]) = xW_x + yW_y + zW_z
        # Let's handle this at the Input Fusion level, not just Attention level.
        
        # HOWEVER, for Attention specifically:
        # If we treat x as "Memory" and y, z as "Query/State", 
        # we can perform Cross-Attention to x + Self-Attention to y,z.
        # But simpler is: Just reuse the computed Attention Keys/Values for x part if architecture allows.
        
        # Let's go with the merged approach for simple implementation but efficient compute:
        # We assume the input to this layer is ALREADY fused. 
        # To truly solve "KV Mismatch" efficiently, we should separate x processing.
        pass 

# --- RE-IMPLEMENTATION WITH CORRECT OPTIMIZATION ---

class EfficientTRMBlock(nn.Module):
    def __init__(self, d_lat, num_heads):
        super().__init__()
        self.d_lat = d_lat
        self.norm = nn.RMSNorm(d_lat)
        
        # Fusion is split: 
        # x_proj is computed ONCE per global step.
        # yz_proj is computed EVERY recursion step.
        self.x_proj = nn.Linear(d_lat, d_lat, bias=False) 
        self.yz_proj = nn.Linear(2 * d_lat, d_lat, bias=False) # takes [y; z]
        
        # Standard Attention Mechanism
        self.attn = nn.MultiheadAttention(d_lat, num_heads, batch_first=True)
        
        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(d_lat, 4 * d_lat),
            nn.SiLU(),
            nn.Linear(4 * d_lat, d_lat)
        )
        self.norm_ffn = nn.RMSNorm(d_lat)

    def forward(self, x_precomputed, y, z):
        # 1. Efficient Input Fusion
        # x_precomputed = x @ W_x (Cached)
        # yz_features = [y; z] @ W_yz (Computed now)
        fused_input = x_precomputed + self.yz_proj(torch.cat([y, z], dim=-1))
        
        # 2. Attention
        # Here we perform Self-Attention on the fused state.
        # Note: We do NOT cache KV here across recursions because 'fused_input' changes entirely.
        # The optimization is in the "Input Fusion" step above.
        h = self.norm(fused_input)
        attn_out, _ = self.attn(h, h, h, need_weights=False, is_causal=True)
        h = fused_input + attn_out
        
        # 3. FFN
        out = h + self.ffn(self.norm_ffn(h))
        return out

class TinyRecursiveTransformer(nn.Module):
    def __init__(self, d_lat=1024, num_heads=16, n_recursion=6):
        super().__init__()
        self.d_lat = d_lat
        self.n = n_recursion
        
        # The Optimized Block
        self.block = EfficientTRMBlock(d_lat, num_heads)

    def precompute_x(self, x):
        """
        Called once before the recursion loop starts.
        Computes the static contribution of x to the fusion layer.
        """
        return self.block.x_proj(x)

    def forward_recursion_process(self, x, y, z):
        # Precompute static part
        x_static = self.precompute_x(x)
        
        curr_z = z
        
        # 1. Inner Loop (Update z)
        for _ in range(self.n):
            # No redundant projection of x inside here
            delta_z = self.block(x_static, y, curr_z)
            curr_z = curr_z + delta_z

        # 2. Outer Loop Step (Update y)
        # Masking x implies using zero-vector contribution from x
        x_dummy_static = torch.zeros_like(x_static) 
        delta_y = self.block(x_dummy_static, y, curr_z)
        y_new = y + delta_y
        
        return y_new, curr_z
4. 수정된 설계의 핵심 이점이 "Split Projection Fusion" 방식은 사용자님이 우려하신 KV 불일치 및 연산 비효율 문제를 다음과 같이 해결합니다.연산 분리: $3 \times D_{lat}$ 크기의 입력을 매번 융합하는 대신, 불변하는 $x$(1/3 비중)의 연산을 루프 밖으로 빼냈습니다.KV Cache 불필요: 내부 상태($z$)가 근본적으로 변하기 때문에 KV Cache를 억지로 유지하려다 발생하는 State Drift 문제를 원천 차단했습니다. 대신 **Fusion Cache ($x_{static}$)**를 도입하여, 실질적인 연산 이득(Projection 연산 $n$번 생략)은 챙기면서 수학적 정합성을 유지했습니다.이제 Part 2의 구현이 Qwen(Encoder) $\rightarrow$ Precompute $x$ $\rightarrow$ Recursive Loop의 효율적인 파이프라인으로 완성되었습니다.

