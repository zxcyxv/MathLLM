{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\nimport torch\n\nprint('=' * 50)\nprint('AIMO3 H100 GPU Test via API')\nprint('=' * 50)\n\nresult = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\nprint(result.stdout)\n\nif torch.cuda.is_available():\n    name = torch.cuda.get_device_name(0)\n    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f'GPU: {name}')\n    print(f'Memory: {mem:.1f} GB')\n    if 'H100' in name:\n        print('\\n>>> SUCCESS: H100 via Kaggle API!')\n    else:\n        print(f'\\n>>> Got: {name}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T14:56:11.854128Z","iopub.execute_input":"2025-12-08T14:56:11.854234Z","iopub.status.idle":"2025-12-08T14:56:14.46386Z","shell.execute_reply.started":"2025-12-08T14:56:11.854221Z","shell.execute_reply":"2025-12-08T14:56:14.463375Z"}},"outputs":[],"execution_count":null}]}