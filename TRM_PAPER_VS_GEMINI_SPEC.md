# TRM ë…¼ë¬¸ vs Gemini ëª…ì„¸ì„œ ì°¨ì´ì  ë¶„ì„

## ê°œìš”

ì´ ë¬¸ì„œëŠ” **TRM ë…¼ë¬¸** (arXiv:2510.04871, "Less is More: Recursive Reasoning with Tiny Networks")ê³¼
**Geminiê°€ ì‘ì„±í•œ ëª…ì„¸ì„œ** (Part1.txt, Part2.txt, Part3.txt)ì˜ ëª¨ë“  ì°¨ì´ì ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## 1. ì…ë ¥ ìœµí•© ë°©ì‹ (Input Fusion)

### ğŸ“„ TRM ë…¼ë¬¸
```python
# ë§ì…ˆ (Addition)
z = net(x + y + z)   # z ì—…ë°ì´íŠ¸
y = net(y + z)       # y ì—…ë°ì´íŠ¸
```

**ê·¼ê±°**:
- Page 2: `zL â† fL(zL + zH + x)` (HRM ì„¤ëª…)
- Page 6: "since z â† fL(x + y + z) contains x but y â† fH(y + z) does not contains x"
- Figure 1: âŠ• (ë§ì…ˆ) ê¸°í˜¸ë¡œ x, y, z ì—°ê²°

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
# Concatenation + Linear Projection
combined = torch.cat([x, y, z], dim=-1)  # [B, S, 3*D]
output = self.net(combined)               # [B, S, D]
```

**ìœ„ì¹˜**: Part2.txt Line 50-52, 104, 205-210

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| ë°©ì‹ | `x + y + z` (element-wise addition) | `concat([x,y,z])` â†’ Linear |
| ì…ë ¥ ì°¨ì› | D â†’ D | 3D â†’ D |
| íŒŒë¼ë¯¸í„° | ì—†ìŒ (ê°™ì€ ì°¨ì›) | Linear(3D, D) í•„ìš” |

---

## 2. y ì—…ë°ì´íŠ¸ ì‹œ x ì²˜ë¦¬ ë°©ì‹

### ğŸ“„ TRM ë…¼ë¬¸
```python
y = net(y, z)  # xë¥¼ ì•„ì˜ˆ ì…ë ¥í•˜ì§€ ì•ŠìŒ
```

**ê·¼ê±°**:
- Figure 3 (Page 5): `y = net(y, z)  # refine output answer`
- Page 6: "y â† fH(y + z) does **not contains x**"
- Page 6: "the task to achieve... is directly specified by the **inclusion or lack of x** in the inputs"

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
x_dummy = torch.zeros_like(x)
y_new = self._single_step(x_dummy, y, curr_z)  # xë¥¼ 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
```

**ìœ„ì¹˜**: Part2.txt Line 93-97, 232-236, 388-390

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| x ì²˜ë¦¬ | ì…ë ¥ì—ì„œ ì™„ì „íˆ ì œì™¸ | zeros_like(x)ë¡œ ë§ˆìŠ¤í‚¹ |
| ìˆ˜ì‹ | `net(y + z)` | `net(0 + y + z)` |
| ì˜ë¯¸ | ëª¨ë¸ì´ x ì—†ì´ ì¶”ë¡ í•˜ë„ë¡ ê°•ì œ | 0 ë²¡í„°ê°€ ì—¬ì „íˆ concatë¨ |

---

## 3. Gradient ì²˜ë¦¬ (Deep Supervision)

### ğŸ“„ TRM ë…¼ë¬¸
```python
def deep_recursion(x, y, z, n=6, T=3):
    # T-1íšŒëŠ” gradient ì—†ì´ ì‹¤í–‰
    with torch.no_grad():
        for j in range(T-1):
            y, z = latent_recursion(x, y, z, n)

    # ë§ˆì§€ë§‰ 1íšŒë§Œ gradientë¡œ ì‹¤í–‰
    y, z = latent_recursion(x, y, z, n)

    return (y.detach(), z.detach()), output_head(y)
```

**ê·¼ê±°**: Figure 3 (Page 5)

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
for step in range(T):
    y, z = self.engine.forward_recursion_process(x, y, z)

    # ë§¤ stepë§ˆë‹¤ gradient ê³„ì‚°
    loss = self.ce_loss(logits, labels)
    total_loss += loss

    # step ëë‚  ë•Œ detach
    y = y.detach()
    z = z.detach()
```

**ìœ„ì¹˜**: Part3.txt Line 70-103

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| Gradient ë²”ìœ„ | ë§ˆì§€ë§‰ 1íšŒ full recursionë§Œ | ë§¤ stepì˜ n+1 recursion |
| no_grad ì‚¬ìš© | T-1íšŒ ì „ì²´ë¥¼ no_gradë¡œ ê°ìŒˆ | ì‚¬ìš© ì•ˆ í•¨ |
| Loss ê³„ì‚° | ë§ˆì§€ë§‰ stepë§Œ | ëª¨ë“  step ëˆ„ì  |
| detach ì‹œì  | return ì‹œ í•œ ë²ˆ | ë§¤ step ë |

---

## 4. Deep Supervision Loop êµ¬ì¡°

### ğŸ“„ TRM ë…¼ë¬¸
```python
# Figure 3 Deep Supervision Loop
for x_input, y_true in train_dataloader:
    y, z = y_init, z_init
    for step in range(N_supervision):
        x = input_embedding(x_input)
        (y, z), y_hat, q_hat = deep_recursion(x, y, z)

        loss = softmax_cross_entropy(y_hat, y_true)
        loss += binary_cross_entropy(q_hat, (y_hat == y_true))

        # ë§¤ stepë§ˆë‹¤ backward & update
        loss.backward()
        opt.step()
        opt.zero_grad()

        if q_hat > 0:  # early-stopping
            break
```

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
for step in range(steps):
    y, z = self.engine.forward_recursion_process(x, y, z)
    logits, halt_logit = self.heads(y)

    # Loss ëˆ„ì 
    total_loss += step_ce_loss + 0.1 * step_halt_loss

    # detachë§Œ í•˜ê³  backwardëŠ” ë‚˜ì¤‘ì—
    y = y.detach()
    z = z.detach()

# ë£¨í”„ ëë‚˜ê³  í•œ ë²ˆì— backward (ë˜ëŠ” ë§¤ step backward)
```

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| backward ì‹œì  | ë§¤ supervision stepë§ˆë‹¤ | ëª¨í˜¸í•¨ (ëˆ„ì  í›„ ë˜ëŠ” ë§¤ë²ˆ) |
| optimizer step | ë§¤ supervision stepë§ˆë‹¤ | ë£¨í”„ ëë‚˜ê³  í•œ ë²ˆ |
| Early stopping | ACT ê¸°ë°˜ early stopping | ì—†ìŒ (ê³ ì • T steps) |

---

## 5. ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (Architecture)

### ğŸ“„ TRM ë…¼ë¬¸
```python
# ë‹¨ìˆœ 2-layer êµ¬ì¡° (Less is More)
self.net = nn.Sequential(
    SwiGLU(D, 4*D, D),
    RMSNorm(D)
)
# ë˜ëŠ” Self-Attention + MLP (ARC-AGIìš©)
```

**ê·¼ê±°**:
- Page 7: "using 2 layers (instead of 4 layers) maximized generalization"
- Table 1: "TRM (T=3, n=6)" uses 2 layers, 5M params
- Page 7: "Less is More" - ì‘ì€ ë„¤íŠ¸ì›Œí¬ê°€ ì˜¤ë²„í”¼íŒ… ë°©ì§€

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
# Split Projection Fusion + Transformer Block
class EfficientTRMBlock(nn.Module):
    def __init__(self, d_lat, num_heads):
        self.x_proj = nn.Linear(d_lat, d_lat)      # xìš© ë³„ë„ projection
        self.yz_proj = nn.Linear(2*d_lat, d_lat)   # [y;z]ìš© ë³„ë„ projection
        self.attn = nn.MultiheadAttention(...)
        self.ffn = nn.Sequential(...)
```

**ìœ„ì¹˜**: Part2.txt Line 317-356

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| êµ¬ì¡° | ë‹¨ìˆœ 2-layer (SwiGLU + Norm) | Split Projection + Transformer |
| íŒŒë¼ë¯¸í„° | 5-7M | ë” ë§ìŒ (ë³µì¡í•œ êµ¬ì¡°) |
| ì² í•™ | "Less is More" - ìµœì†Œí™” | KV-Cache ìµœì í™” - íš¨ìœ¨í™” |
| x ì²˜ë¦¬ | ë§ì…ˆìœ¼ë¡œ í†µí•© | ë³„ë„ projection í›„ í•©ì‚° |

---

## 6. precompute_x ìµœì í™”

### ğŸ“„ TRM ë…¼ë¬¸
**ì—†ìŒ** - ë§ì…ˆ ë°©ì‹ì´ë¼ xë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•  í•„ìš” ì—†ìŒ

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
def precompute_x(self, x):
    """ë£¨í”„ ì „ì— í•œ ë²ˆë§Œ ê³„ì‚°"""
    return self.block.x_proj(x)

def forward(self, x_static, y, z):
    # x_staticì€ ë¯¸ë¦¬ ê³„ì‚°ëœ ê°’
    fused_input = x_static + self.yz_proj(torch.cat([y, z], dim=-1))
```

**ìœ„ì¹˜**: Part2.txt Line 367-372

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| x ìºì‹± | ë¶ˆí•„ìš” (ë§ì…ˆ) | x_proj ê²°ê³¼ ìºì‹± |
| ëª©ì  | - | ì—°ì‚° íš¨ìœ¨í™” |
| ë³µì¡ë„ | ë‹¨ìˆœ | ì¶”ê°€ ë¡œì§ í•„ìš” |

---

## 7. Residual Connection

### ğŸ“„ TRM ë…¼ë¬¸
```python
# Figure 3 pseudocode - ì§ì ‘ êµì²´
for i in range(n):
    z = net(x, y, z)  # zë¥¼ ì§ì ‘ êµì²´
y = net(y, z)         # yë¥¼ ì§ì ‘ êµì²´
```

**ì°¸ê³ **: Figure 1ì˜ ë‹¤ì´ì–´ê·¸ë¨ì—ëŠ” "Add & Norm"ì´ í‘œì‹œë˜ì–´ ìˆì–´ residualì´ ìˆì„ ìˆ˜ë„ ìˆìŒ

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
for _ in range(self.n):
    delta_z = self._single_step(x, y, curr_z)
    curr_z = curr_z + delta_z  # ëª…ì‹œì  residual

delta_y = self._single_step(x_dummy, y, curr_z)
y_new = y + delta_y  # ëª…ì‹œì  residual
```

**ìœ„ì¹˜**: Part2.txt Line 76-97, 221-236

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| z ì—…ë°ì´íŠ¸ | `z = net(...)` (êµì²´) | `z = z + net(...)` (residual) |
| y ì—…ë°ì´íŠ¸ | `y = net(...)` (êµì²´) | `y = y + net(...)` (residual) |
| ê·¼ê±° | Pseudocode ê¸°ì¤€ | "gradient stability" ì–¸ê¸‰ |

---

## 8. ACT (Adaptive Computation Time)

### ğŸ“„ TRM ë…¼ë¬¸
```python
# ë‹¨ìˆœí™”ëœ ACT - Binary CEë§Œ ì‚¬ìš©
loss += binary_cross_entropy(q_hat, (y_hat == y_true))

if q_hat > 0:  # early-stopping
    break
```

**ê·¼ê±°**:
- Page 7: "get rid of the continue loss (from the Q-learning)"
- Page 7: "only learn a halting probability through a Binary-Cross-Entropy loss"
- Table 1: "w/ ACT" (86.1%) vs without (87.4%) - ACT ì—†ì´ë„ ì¢‹ìŒ

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
# Q-learning ê¸°ë°˜ ACT (HRM ë°©ì‹)
class TRMHeads(nn.Module):
    self.halt_head = nn.Sequential(
        nn.Linear(d_lat, d_lat // 2),
        nn.SiLU(),
        nn.Linear(d_lat // 2, 1)
    )

# Halting loss
halt_target = (accuracy > 0.99).float()
step_halt_loss = self.bce_loss(halt_logit.mean(), halt_target)
```

**ìœ„ì¹˜**: Part3.txt Line 1-18, 87-98

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| ë°©ì‹ | ë‹¨ìˆœ BCE | Q-learning ê¸°ë°˜ |
| forward pass | 1íšŒ | 2íšŒ (continue lossìš©) |
| êµ¬í˜„ | ë‹¨ìˆœ | ë³µì¡ |
| í•„ìš”ì„± | ì„ íƒì  (ì—†ì–´ë„ ë¨) | í•„ìˆ˜ì²˜ëŸ¼ ì„œìˆ  |

---

## 9. EMA (Exponential Moving Average)

### ğŸ“„ TRM ë…¼ë¬¸
```
EMA = 0.999
```

**ê·¼ê±°**:
- Page 7: "integrate Exponential Moving Average (EMA) of the weights"
- Page 7: "going from 79.9% to 87.4%; see Table 1"
- Page 11: "TRM uses an Exponential Moving Average (EMA) of 0.999"

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
**ì–¸ê¸‰ ì—†ìŒ**

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| EMA | 0.999 ì‚¬ìš© | ì–¸ê¸‰ ì—†ìŒ |
| íš¨ê³¼ | +7.5% accuracy | - |
| ì•ˆì •ì„± | ì˜¤ë²„í”¼íŒ…/ë°œì‚° ë°©ì§€ | ê³ ë ¤ ì•ˆ ë¨ |

---

## 10. ìƒíƒœ ë³€ìˆ˜ í•´ì„ (State Variables)

### ğŸ“„ TRM ë…¼ë¬¸
- **x**: Input question (embedded)
- **y**: Current proposed solution (= zH in HRM)
- **z**: Latent reasoning feature (= zL in HRM)

**ê·¼ê±°** (Page 6):
> "zH is simply the current (embedded) solution... zL is a latent feature that does not directly correspond to a solution"
> "hierarchy is not needed; there is simply an input x, a proposed solution y, and a latent reasoning feature z"

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
- **x**: Context State (Invariant Semantic Representation)
- **y**: Solution State (ì ì •ì  ì •ë‹µ ì„ë² ë”©)
- **z**: Reasoning State (ë…¼ë¦¬ì  ì‚¬ê³ ì˜ ê¶¤ì )

**ìœ„ì¹˜**: Part1.txt "B. State Variables Definition"

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| x ì—­í•  | ì…ë ¥ ì§ˆë¬¸ | ë¶ˆë³€ì˜ ë¬¸ë§¥ (Anchor) |
| y ì—­í•  | í˜„ì¬ í•´ë‹µ | ì ì •ì  ì •ë‹µ |
| z ì—­í•  | ì¶”ë¡  ê³¼ì • (CoT ìœ ì‚¬) | Hidden Reasoning Path |
| í•´ì„ | ë‹¨ìˆœ (ê³„ì¸µ ì—†ìŒ) | ë³µì¡ (Information Bottleneck) |

---

## 11. í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ğŸ“„ TRM ë…¼ë¬¸ (Page 11)
| Parameter | Value |
|-----------|-------|
| n (inner recursion) | 6 |
| T (supervision steps) | 3 |
| Hidden size | 512 |
| Batch size | 768 |
| Optimizer | AdamW (Î²1=0.9, Î²2=0.95) |
| Learning rate | 1e-4 |
| Weight decay | 1.0 (Sudoku), 0.1 (ARC) |
| EMA | 0.999 |
| Network layers | 2 |
| Nsup (max) | 16 |

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
| Parameter | Value |
|-----------|-------|
| n_recursion | 6 |
| t_supervision | 3 |
| d_lat | 1024 |
| num_heads | 16 |
| expansion | 4 |
| Network | Transformer Block |

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| Hidden size | 512 | 1024 |
| Network | 2-layer MLP | Transformer |
| EMA | 0.999 | ì—†ìŒ |
| Attention | ì„ íƒì  | í•„ìˆ˜ |

---

## 12. Loss í•¨ìˆ˜

### ğŸ“„ TRM ë…¼ë¬¸
```python
loss = softmax_cross_entropy(y_hat, y_true)
loss += binary_cross_entropy(q_hat, (y_hat == y_true))
```

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
# CE Loss
step_ce_loss = self.ce_loss(shift_logits, shift_labels)

# Halting Loss (ë” ë³µì¡)
accuracy = (preds == shift_labels).float().mean()
halt_target = (accuracy > 0.99).float()
step_halt_loss = self.bce_loss(halt_logit.mean(), halt_target)

total_loss += step_ce_loss + 0.1 * step_halt_loss
```

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| CE Loss | ë‹¨ìˆœ CE | Shifted CE (Causal LM) |
| Halt target | `y_hat == y_true` (ì •í™• ì¼ì¹˜) | `accuracy > 0.99` (ì„ê³„ê°’) |
| Halt weight | 1.0 (ë™ì¼) | 0.1 |

---

## 13. Self-Attention vs MLP

### ğŸ“„ TRM ë…¼ë¬¸
```python
# Sudoku-Extreme (9x9): MLPê°€ ë” ì¢‹ìŒ
# Maze-Hard, ARC-AGI (30x30): Self-Attentionì´ ë” ì¢‹ìŒ
```

**ê·¼ê±°** (Page 7):
> "Using an MLP instead of self-attention, we obtain better generalization on Sudoku-Extreme (improving from 74.7% to 87.4%)"
> "we found this architecture to be suboptimal for tasks with large context length"

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
- Self-Attentionì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
- MLP ëŒ€ì•ˆ ì–¸ê¸‰ ì—†ìŒ

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| ì ‘ê·¼ | Taskë³„ ì„ íƒ | Self-Attention ê³ ì • |
| ì‘ì€ context | MLP ì¶”ì²œ | - |
| í° context | Self-Attention | Self-Attention |

---

## 14. ì´ˆê¸°í™” ì „ëµ

### ğŸ“„ TRM ë…¼ë¬¸
```python
y, z = y_init, z_init  # í•™ìŠµ ê°€ëŠ¥í•œ ì´ˆê¸°í™”
```

**ê·¼ê±°**: Figure 3 - `y, z = y_init, z_init`

### ğŸ“‹ Gemini ëª…ì„¸ì„œ
```python
y_0 = self.y_init.expand(batch_size, seq_len, -1)  # í•™ìŠµ ê°€ëŠ¥

if self.z_init_strategy == "copy_x":
    z_0 = x_latent.clone()  # x ë³µì‚¬
else:
    z_0 = torch.zeros_like(x_latent)
```

**ìœ„ì¹˜**: Part1.txt Line 41-56

### âš ï¸ ì°¨ì´ì 
| í•­ëª© | ë…¼ë¬¸ | Gemini |
|------|------|--------|
| y ì´ˆê¸°í™” | í•™ìŠµ ê°€ëŠ¥ ë²¡í„° | í•™ìŠµ ê°€ëŠ¥ (0 ê¸°ë°˜) |
| z ì´ˆê¸°í™” | í•™ìŠµ ê°€ëŠ¥ ë²¡í„° | x ë³µì‚¬ ë˜ëŠ” 0 |

---

## ìš”ì•½: í•µì‹¬ ì°¨ì´ì  TOP 5

### 1ï¸âƒ£ ì…ë ¥ ìœµí•©
- **ë…¼ë¬¸**: `x + y + z` (ë§ì…ˆ)
- **Gemini**: `concat([x,y,z])` â†’ Linear

### 2ï¸âƒ£ y ì—…ë°ì´íŠ¸
- **ë…¼ë¬¸**: `net(y, z)` - x ì—†ìŒ
- **Gemini**: `net(0, y, z)` - xë¥¼ 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹

### 3ï¸âƒ£ Gradient ì²˜ë¦¬
- **ë…¼ë¬¸**: T-1íšŒ no_grad + 1íšŒ grad
- **Gemini**: ë§¤ step gradient í›„ detach

### 4ï¸âƒ£ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
- **ë…¼ë¬¸**: ë‹¨ìˆœ 2-layer ("Less is More")
- **Gemini**: Split Projection + Transformer

### 5ï¸âƒ£ EMA
- **ë…¼ë¬¸**: 0.999 (í•„ìˆ˜ì , +7.5% ì„±ëŠ¥)
- **Gemini**: ì–¸ê¸‰ ì—†ìŒ

---

## ê¶Œì¥ ì‚¬í•­

**ë…¼ë¬¸ëŒ€ë¡œ ì¬êµ¬í˜„**í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤:

1. ì…ë ¥ ìœµí•©ì„ ë§ì…ˆ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
2. y ì—…ë°ì´íŠ¸ ì‹œ xë¥¼ ì™„ì „íˆ ì œì™¸
3. T-1íšŒ no_grad + 1íšŒ grad êµ¬ì¡° ì ìš©
4. ë‹¨ìˆœ 2-layer ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© ê³ ë ¤
5. EMA 0.999 ì¶”ê°€
6. ë§¤ supervision stepë§ˆë‹¤ backward/step ìˆ˜í–‰

---

## ì°¸ê³  ìë£Œ

- **TRM ë…¼ë¬¸**: arXiv:2510.04871
- **Gemini ëª…ì„¸ì„œ**: Part1.txt, Part2.txt, Part3.txt
- **TRM GitHub**: (cloneëœ ì½”ë“œ ì°¸ì¡°)
